---
layout: post
title: All of vector calculus and linear algebra
categories: blog
permalink: blog/vector-calculus-and-linear-algebra
mathjax: true
---

<!--more-->

This is a big reference sheet/summary for Theodore Shifrin's Multivariable Mathematics textbook. Much of this was written while taking math 3510 as a freshman in college, which I still consider to be the hardest class I have ever taken. This is not meant to be used to learn from. Actually, this would be a terrible reference to learn from. Instead, it is meant to go over the major points and key ideas of the course, and some fun stuff, mostly as a reference to look back on when studying or after taking the course :)

This is currently unfinished and I go and add stuff to it every once in a while to make it more complete.

---
<!-- # Chapter 1: Vectors and Matrices
## 1.1 Vectors in $\mathbb{R}^n$
// TODO
## 1.2 Dot Product
// TODO
## 1.3 Subspaces of $\mathbb{R}^n$
// TODO
## 1.4 Linear Transformations and Matrix Algebra
// TODO
## 1.5 Introduction to Determinants and the Cross Product
// TODO

---
# Chapter 2: Functions, Limits, and Continuity
## 2.1 Scalar- and Vector-Valued Functions
// TODO
## 2.2 A Bit of Topology in $\mathbb{R}^n$
// TODO
## 2.3 Limits and Continuity
// TODO

---
# Chapter 3: The Derivative
## 3.1 Partial Derivatives and Directional Derivatives
// TODO
## 3.2 Differentiability
// TODO
## 3.3 Differentiation Rules
// TODO
## 3.4 The Gradient
// TODO
## 3.5 Curves
// TODO
## 3.6 Higher-Order Partial Derivatives
// TODO

---
# Chapter 4: Implicit and Explicit Solutions of Linear Systems
## 4.1 Gaussian Elimination and the Theory of Linear Systems
// TODO
## 4.2 Elementary Matrices and Calculating Inverse Matrices
// TODO
## 4.3 Linear Independence, Basis, and Dimension
// TODO
## 4.4 The Four Fundamental Subspaces
// TODO
## 4.5 The Nonlinear Case: Introduction to Manifolds
// TODO

---
# Chapter 5: Extremum Problems
## 5.1 Compactness and the Maximum Value Theorem
// TODO
## 5.2 Maximum/Minimum Problems
// TODO
## 5.3 Quadratic Forms and the Second Derivative Test
// TODO
## 5.4 Lagrange Multipliers
// TODO
## 5.5 Projections, Least Squares, and Inner Product Spaces
// TODO

---
-->
# Chapter 6: Solving Nonlinear Problems

## 6.1 The Contraction Mapping Principle

### Convergence of Vector Sums
If you have a convergent sequence of vectors $\{\boldsymbol{a}\_k\}$, then this convergent sequence of vectors will converge as long as the norms of the vectors converge
$\sum\limits\_{k=1}^\infty \|\boldsymbol{a}\_k\|$ converges $\implies$ $\sum\limits\_{k=1}^\infty \boldsymbol{a}\_k$ converges

### Contraction Mappings
A contraction mapping is a function that always shrinks the distance between two points.
For $X \subset \mathbb{R}^n, \boldsymbol{f}: X \to X$, $\boldsymbol{f}$ is a contraction mapping if $\exists c \in (0, 1)$ such that $$\| \boldsymbol{f}(\boldsymbol{x}) - \boldsymbol{f}(\boldsymbol{y})\| \leq c \| \boldsymbol{x} - \boldsymbol{y}\|$$
Contraction mappings are always continuous, and have at most one fixed point. If $X$ is closed, we can specify that such a point does exist. This is called the contraction mapping principle. The contraction mapping principle states that if you call a contraction mapping $\boldsymbol{f}$ enough times, then it eventually converges on some point, called the fixed point of $\boldsymbol{f}$.
if $X$ is closed, then $\exists \boldsymbol{x} \in X$ so that $\boldsymbol{f}(\boldsymbol{x})=\boldsymbol{x}$

### The Mean Value Inequality
The MVT in single-var calc tells us that, on a continuous curve, there is some point where the tangent line at that point has the same slope as the secant line between the two endpoints. i.e. that  $\cfrac{f(b)-f(a)}{b-a} = f'(x)$ for some $x \in (a, b)$. If we rearrange, however, this result tells us how much a function stretches or shrinks two points based on the slope of the function. This idea is is how we generalize the MVT to $\mathbb{R}^n$
If $U \subset \mathbb{R}^n$ is open,  $\boldsymbol{f}: U \to \mathbb{R}^m$, and $\boldsymbol{a}, \boldsymbol{b} \in U$ with a ${\mathscr{C}^1}$ path in U which joins them, then
$$\| \boldsymbol{f}(\boldsymbol{b}) - \boldsymbol{f}(\boldsymbol{a})\| \leq \left( \displaystyle \max\_{\boldsymbol{x} \in [\boldsymbol{a}, \boldsymbol{b}]}\| D\boldsymbol{f}(\boldsymbol{x})\|\right)\|\boldsymbol{b}-\boldsymbol{a}\|.$$
This gives us a really useful condition which we can use to prove that certain functions are contraction mappings. <br> (If $0< \displaystyle \max\_{\boldsymbol{x} \in [\boldsymbol{a}, \boldsymbol{b}]}\| D\boldsymbol{f}(\boldsymbol{x})\| < 1$).

### Matrix Theorems
Generalization of a geometric series. If $A\in\mathcal{M}\_{n \times n}$ and $\|A\|\_{\scriptscriptstyle\text{op}}< 1$, then $\sum\limits\_{i=0}^\infty A^i=(I-A)^{-1}$
The general linear group $\text{GL}(n)$ of invertible matrices is an open subset of $\mathcal{M}\_{n \times n}$, and $f: \text{GL}(n) \to  \text{GL}(n)$ given by $f(A) = A^{-1}$ is smooth.
Since $\mathcal{M}\_{m \times n}$ is isomorphic to $\mathbb{R}^{nm}$, we can take derivative maps of matrix-valued functions. For example, consider $f: \mathcal{M}\_{n\times n} \to \mathcal{M}\_{n\times n}$ given by $f(A) = A^2$. We can use the limit definition similarly to vectors (with respect to the frobenius norm) and show $[Df(A)]B = AB + BA$. What this says, is that if we associate a linear map to $[Df(A)]: \mathbb{R}^{n^2} \to \mathbb{R}^{n^2}$, then that linear map takes a $B \in \mathbb{R}^{n^2}$ to $AB + BA$ (a sort of matrix power rule, which unfortunately doesn't simplify since multiplication is non-commutative).
The symmetric matrices where $A^T = A$ form an $\frac{1}{2}n(n+1)$-dimensional subspace. The orthogonal group $\text{O}(n) \subset \mathcal{M}\_{n \times n}$ = {set of matrices where $A^TA = I$}  is a $\frac{1}{2}n(n-1)$-dimensional submanifold, and $T\_I\text{O}(n)$ (tangent plane at the identity) is the set of skew-symmetric matrices $\text{Skew}(n)$ = {set of matrices where $A^T = -A$}

## 6.2 The Inverse and Implicit Function Theorems

### Inverse Function Theorem
The Inv. FT is an incredibly powerful theorem which tells us under what conditions a certain function $\boldsymbol{f}$ has a local inverse, and what the derivative of that inverse looks like. To have an inverse, the derivative map must be locally invertible, and the inverse of that derivative map is the derivative of the inverse.
More formally it says if $U \subset \mathbb{R}^n$ is open,  $\boldsymbol{f}: U \to \mathbb{R}^n$ is ${\mathscr{C}^1}$, $\boldsymbol{x}\_0 \in U$ and $D\boldsymbol{f}(\boldsymbol{x}\_0)$ is invertible, then $\exists$ two open neighborhoods, $V \subset U$ of $\boldsymbol{x}\_0$ and $W$ of $\boldsymbol{y}\_0 := \boldsymbol{f}(\boldsymbol{x}\_0)$, and a ${\mathscr{C}^1}$ function between them $\boldsymbol{g}: V \to W$ so that
$\forall \boldsymbol{x} \in V, \boldsymbol{g}(\boldsymbol{f}(\boldsymbol{x})) = \boldsymbol{x}$
$\forall \boldsymbol{y} \in W, \boldsymbol{f}(\boldsymbol{g}(\boldsymbol{y})) = \boldsymbol{y}$
Furthermore, $D\boldsymbol{g}(\boldsymbol{f}(\boldsymbol{x})) = (D\boldsymbol{f}(\boldsymbol{x}))^{-1}$

### Implicit Function Theorem
The Imp. FT, a consequence of the Inv. FT, is another incredibly powerful result. It tells us that a series of $m$ implicit equations of $n$ variables can be written locally as $m$ explicit functions of $n-m$ variables (as long as the derivative map of the output variables is invertible). It is like a non-linear generalization of pivot and free variables. This is also incredibly useful for our future study of manifolds; it links the implicit description to the explicit description and allows us to think of manifolds locally as spaces where we can perform calculus like it's linear algebra.
More formally, for the Imp. FT we need $n>m$, and a  ${\mathscr{C}^1}$ function $\boldsymbol{F}: U \subset \mathbb{R}^n \to \mathbb{R}^m$ where $U$ is open. Then, writing a vector $\begin{bmatrix}\boldsymbol{x} \\\\ \boldsymbol{y}\\ \end{bmatrix} \in \mathbb{R}^n$ with $\boldsymbol{x} \in \mathbb{R}^{n-m}$ and $\boldsymbol{y} \in \mathbb{R}^m$, suppose $\boldsymbol{F}\begin{pmatrix} \boldsymbol{x}\_0 \\ \boldsymbol{y}\_0 \\ \end{pmatrix} = \boldsymbol{\vec{0}}$ describes a series of $m$ implicit equations and $\cfrac{\partial \boldsymbol{F}}{\partial \boldsymbol{y}} \begin{pmatrix} \boldsymbol{x}\_0 \\ \boldsymbol{y}\_0 \\ \end{pmatrix} \in \mathcal{M}\_{m \times m}$ is invertible.
Then, $\exists$ two neighborhoods, $V$ of $\boldsymbol{x}\_0$ and $W$ of $\boldsymbol{y}\_0$ and a ${\mathscr{C}^1}$ function between them $\boldsymbol{\varphi}: V \to W$ so that for all $\boldsymbol{x} \in V$ and $\boldsymbol{y} \in W$,
$$\boldsymbol{F}\begin{pmatrix} \boldsymbol{x} \\ \boldsymbol{y} \\ \end{pmatrix} = \boldsymbol{\vec{0}} \iff \boldsymbol{y} = \boldsymbol{\varphi}(\boldsymbol{x})$$
Furthermore, $D\boldsymbol{\varphi}(\boldsymbol{x})=-\left(\cfrac{\partial \boldsymbol{F}}{\partial \boldsymbol{y}}\begin{pmatrix} \boldsymbol{x} \\ \boldsymbol{\varphi}(\boldsymbol{x})\\ \end{pmatrix}\right)^{-1}\left(\cfrac{\partial \boldsymbol{F}}{\partial \boldsymbol{x}}\begin{pmatrix} \boldsymbol{x} \\ \boldsymbol{\varphi}(\boldsymbol{x})\\ \end{pmatrix}\right)$.
It helps to keep track of dimensions. $\cfrac{\partial \boldsymbol{F}}{\partial \boldsymbol{y}}$ is an $m \times m$ matrix (differentiating *m* functions w.r.t *m* variables, $\cfrac{\partial \boldsymbol{F}}{\partial \boldsymbol{x}}$ is an $m \times (n-m)$ matrix (differentiating *m* functions w.r.t. *(n-m)* variables, so $D\boldsymbol{\varphi}(\boldsymbol{x}) \in \mathcal{M}\_{m \times (n-m)}$.

## 6.3 Manifolds Revisited
Manifolds are essentially special surfaces which are smooth and are very nice to deal with when doing calculus.
There are three equivalent definitions for a k-dimensional manifold *M*.

1. **Explicit:** a graph of a smooth function
2. **Implicit:** solutions to an implicit set of equations
3. **Parametric:** parameterized by an injective function that never has velocity 0.


more formally
1. For any $\boldsymbol{p} \in M$, $\exists$ a neighborhood $W \subset \mathbb{R}^n$ of $\boldsymbol{p}$ so that $W \cap M$ is the graph of a smooth function $\boldsymbol{f}: V \subset \mathbb{R}^{k} \to \mathbb{R}^{n-k}$, where $V$ is open. Remember that $\text{graph}\{\boldsymbol{f}(\boldsymbol{x})\} = \{\begin{bmatrix}\boldsymbol{x} \\\\ \boldsymbol{f}(\boldsymbol{x}) \end{bmatrix} : \boldsymbol{x} \in \mathbb{R}^k\}$, but that the order of the $x\_i$'s doesn't matter.
2. For any $\boldsymbol{p} \in M$, $\exists$ a neighborhood $W \subset \mathbb{R}^n$ of $\boldsymbol{p}$ and a smooth function $\boldsymbol{F}: W \to \mathbb{R}^{n-k}$ so that the preimage (level set) $\boldsymbol{F}^{-1}(\boldsymbol{\vec{0}}) = W \cap M$ and $\text{rank}\{D\boldsymbol{F}(\boldsymbol{x})\} = n-k$ for every $\boldsymbol{x} \in W \cap M$.
3. For any $\boldsymbol{p} \in M$, $\exists$ a neighborhood $W \subset \mathbb{R}^n$ of $\boldsymbol{p}$ so that $W \cap M$ is the image of a smooth function $\boldsymbol{g}: U \subset \mathbb{R}^{k} \to \mathbb{R}^{n}$, where $U$ is open. $\boldsymbol{g}$ must be injective, have $\text{rank}\{D\boldsymbol{g}(\boldsymbol{u})\} = k$ for all $\boldsymbol{u} \in U$, and have $\boldsymbol{g}^{-1}: W \cap M \to U$ be continuous.

We can also find the tangent planes at $\boldsymbol{p}$ to each respective manifold ($T\_\boldsymbol{p}M)$ as follows
1. If $M$ is locally at $\boldsymbol{p}$ a graph of $\boldsymbol{f}$ with $\boldsymbol{p} = \begin{bmatrix}\boldsymbol{a} \\\\ \boldsymbol{f}(\boldsymbol{a}) \end{bmatrix}$, then $T\_{\boldsymbol{p}}M = \text{graph}\{D\boldsymbol{f}(\boldsymbol{a})\}$.
2. If $M$ is locally a preimage of $\boldsymbol{F}$,  then $T\_{\boldsymbol{p}}M = \text{ker}\{D\boldsymbol{F}(\boldsymbol{p})\}$.
3. If $M$ is locally parameterized by $\boldsymbol{g}$ with $\boldsymbol{p} = \boldsymbol{g}(\boldsymbol{a})$, then $T\_{\boldsymbol{p}}M = \text{im}\{D\boldsymbol{g}(\boldsymbol{a})\}$

---
# Chapter 7: Integration

## 7.1 Multiple Integrals

### Defining Integrability (in higher dimensions)
To extend our notion of integration into multiple dimensions, we will be working with something called the Darboux integral. It is much like our notion of 2-D area under the curve, but generalizes to volumes of surfaces/hypersurfaces.
Given a rectangle $R \subset \mathbb{R}^n$, you can slice up the rectangle into subrectangles into a partition $\mathcal{P} := \mathcal{P}\_1  \times ... \times \mathcal{P}\_n$ (each $\mathcal{P}\_i$ for $1 \leq i \leq n$ is like a list of points where to slice the rectangle in the corresponding dimension), where each $R\_{i, j}$ of $\mathcal{P}$ is the i-jth rectangle of $R$. Although for $\mathbb{R}^n$ it should be more like the $(i, j, k, ... , n)$-th rectangle of $R$ for all the *n* coordinates needed to specify which subrectangle it is. For ease of notation I'll stick with $R\_{i, j}$.
Recall that for a bounded function $f: R \to \mathbb{R}$, $\displaystyle \sup\_{\boldsymbol{x} \in S} f(\boldsymbol{x})$ is the "least upper bound" and $\displaystyle \inf\_{\boldsymbol{x} \in S} f(\boldsymbol{x})$ is the "greatest lower bound" of *f* on S. That is to say, *sup* is like a maximum of *f* and *inf* is like a minimum of *f* (although the function might never reach their respective values, only approach them).
We then define two possible sums on a partition $\mathcal{P}$, the upper and lower sums. First, we define a sort of minimum/maximum for a subrectangle $R\_{i,j}$ to be  $m\_{i,j} := \displaystyle \inf\_{\boldsymbol{x} \in R\_{i, j}} f(\boldsymbol{x})$ and $M\_{i,j} := \displaystyle \sup\_{\boldsymbol{x} \in R\_{i, j}} f(\boldsymbol{x})$, respectively. Then, the lower sum of a partition is $L(f, \mathcal{P}) := \sum\limits\_{i, j}m\_{i, j}\text{ area}(R\_{i,j})$ and the upper sum is $U(f, \mathcal{P}) := \sum\limits\_{i, j}M\_{i, j}\text{ area}(R\_{i,j})$.
Finally, we define integrability. $f$ is integrable on $R$ if for all partitions $\mathcal{P}$ of $R$ there is a *unique* number $I$ satisfying $L(f, \mathcal{P})\leq I \leq U(f, \mathcal{P})$. If$f$ is integrable we say $\displaystyle \int\_R fdV:= I$.

### Proving Integrability
A nice way to prove integrability is the convenient criterion. If $f$ is bounded on $R$, $f$ is integrable on $R$ iff $\forall \epsilon > 0$ , $\exists$ a partition $\mathcal{P}$ of $R$ such that $U(f, \mathcal{P})-L(f, \mathcal{P}) < \epsilon$.
If $f$ is continuous, and only discontinuous on a set of volume 0, then $f$ is integrable. $X$ having volume 0 means that $\exists$ finite $R\_1, \dots , R\_s$ so that $X \subset R\_1 \cup \dots \cup R\_s$ and $\text{vol}(R\_1) + \dots + \text{vol}(R\_s) < \epsilon$ for any $\epsilon > 0$.

### Regions
A subset $\Omega$ is a region if it is the closure of a bounded open subset of $\mathbb{R}^n$ and its boundary has volume 0. In other words, if $\Omega = \bar{U}$ for some open $U$, and $\partial\Omega$ has volume 0.
To integrate $f$ over a region $\Omega$, we pick a rectangle $R$ with $\Omega \subset R$. Then, we define $\tilde f = \begin{cases} f(\boldsymbol{x}) & x\in \Omega \\ 0 & \text{otherwise} \end{cases}$.
Then, $\displaystyle \int\_\Omega fdV = \displaystyle \int\_R \tilde f dV$
The volume of a region $\Omega$ is given by $\text{vol}(\Omega) = \displaystyle \int\_\Omega 1 dV$

### Properties of Integrals
1. Integral is linear: $\displaystyle \int\_\Omega (f + g) dV = \displaystyle \int\_\Omega fdV + \displaystyle \int\_\Omega gdV$ and $\displaystyle \int\_\Omega \alpha f dV = \alpha\displaystyle \int\_\Omega f dV$
2. Union of sets is sum of integrals: $\displaystyle \int\_{A\cup B} fdV = \displaystyle \int\_{A} fdV + \displaystyle \int\_{B} fdV$
3. Inequalities are maintained: if $f \leq g$ on $\Omega$, then $\displaystyle \int\_\Omega fdV \leq \displaystyle \int\_\Omega gdV$

## 7.2 Iterated Integrals and Fubini's Theorem

### Fubini's Theorem
Fubini's Theorem allows us to connect this abstract definition of integration to our familiar definition of integration in single-variable calculus.
In 2 dimensions, it states that if $f$ is integrable on $R = [a, b] \times [c, d]$, and the sub-integral $\displaystyle\int\_c^d f\begin{pmatrix} x \\\\ y \\ \end{pmatrix}dy$ is integrable, then $\displaystyle\int\_R fdV = \displaystyle\int\_a^b\int\_c^d f\begin{pmatrix} x \\\\ y \\ \end{pmatrix}dy\,dx$. Furthermore, $\displaystyle\int\_a^b\int\_c^d f\begin{pmatrix} x \\\\ y \\ \end{pmatrix}dy\,dx = \displaystyle\int\_c^d\int\_a^b f\begin{pmatrix} x \\\\ y \\ \end{pmatrix}dx\,dy.$
In $n$ dimensions, this generalizes similarly. If $f$ is integrable on $[a\_1, b\_1] \times . . . \times [a\_n, b\_n]$, and all the subintegrals are integrable, then
$\displaystyle\int\_R f(\boldsymbol{x})dV$ = $\displaystyle \int\_{a\_1}^{b\_1} . . . \int\_{a\_n}^{b\_n} f(\boldsymbol{x})dx\_n . . . dx\_1$
and the order of the integrals does not matter.

## 7.3 Polar, Cylindrical, and Spherical Coordinates
There are alternative coordinate systems which can help us solve problems and allow us to exploit symmetry. These are three very specific yet very useful coordinate systems, which we generalize later with the change of variables theorem.

### Polar Coordinates
Define $\boldsymbol{g}: [0, \infty) \times [0, 2\pi) \to \mathbb{R}^2$ by $\boldsymbol{g}\begin{pmatrix} r \\\\ \theta \end{pmatrix} = \begin{bmatrix} r\cos\theta \\ r\sin\theta \end{bmatrix} =  \begin{bmatrix}x \\\\ y \end{bmatrix}$ .

Then, $\displaystyle \int\_{\boldsymbol{g}(\Omega)}f\begin{pmatrix}x \\\\ y\end{pmatrix}dV\_{x, y} = \displaystyle \int\_\Omega f\begin{pmatrix}r \\\\ \theta\end{pmatrix}rdV\_{r, \theta}$
It is useful to know that $x^2 + y^2 = r^2$
Visually, we think of $r$ being the distance projected radially outward at an angle of $\theta$ which rotates clockwise around the origin.

### Cylindrical Coordinates
Define $\boldsymbol{g}: [0, \infty) \times [0, 2\pi) \times \mathbb{R} \to \mathbb{R}^3$ by $\boldsymbol{g}\begin{pmatrix} r \\\\ \theta \\\\ z\end{pmatrix} = \begin{bmatrix} r\cos\theta \\\\ r\sin\theta \\\\z \end{bmatrix} =  \begin{bmatrix}x \\\\ y \\\\ z\end{bmatrix}$ .

Then, $\displaystyle \int\_{\boldsymbol{g}(\Omega)}f\begin{pmatrix}x\\\\y\\\\z\end{pmatrix}dV\_{x, y, z} = \displaystyle \int\_\Omega f\begin{pmatrix}r\\\\\theta\\\\z\end{pmatrix}rdV\_{r, \theta, z}$
It is useful to know that $x^2 + y^2 = r^2$
Visually, we think of $r$ being the distance projected radially outward at an angle of $\theta$ which rotates clockwise around the origin, at a height of $z$.

### Spherical Coordinates
Define $\boldsymbol{g}: [0, \infty) \times [0,\pi) \times [0, 2\pi) \to \mathbb{R}^3$ by $\boldsymbol{g}\begin{pmatrix} \rho \\\\ \phi \\\\ \theta\end{pmatrix} = \begin{bmatrix} \rho\sin\phi\cos\theta \\\\ \rho\sin\phi\sin\theta \\\\ \rho\cos\phi \end{bmatrix} =  \begin{bmatrix}x \\\\ y \\\\ z\end{bmatrix}$ .
Then, $\displaystyle \int\_{\boldsymbol{g}(\Omega)}f\begin{pmatrix}x\\\\y\\\\z\end{pmatrix}dV\_{x, y, z} = \displaystyle \int\_\Omega f\begin{pmatrix} \rho \\\\ \phi \\\\ \theta\end{pmatrix}\rho^2\sin\phi dV\_{\rho, \phi, \theta}$
It is useful to know that $x^2 + y^2 +z^2 = \rho^2$
Visually, we think of $\rho$ being the distance projected radially outward at two anglesâ€”$\phi$, which starts at the top of the $z$-axis and rotates down to the bottom, and $\theta$, which rotates all the way about the $z$-axis.

## 7.4 Physical Applications
Average value of $f$ on $\Omega$ is defined to be $\bar{f} = \cfrac{1}{\text{vol}(\Omega)}\displaystyle\int\_\Omega f \text{ dVol}$
We can define the integral of a vector *valued* function $\boldsymbol{f}$ by $\displaystyle\int\_\Omega \boldsymbol{f} \text{ dVol} = \begin{bmatrix}\int\_\Omega f\_1 \text{ dVol} \\\\ \vdots \\\\ \int\_\Omega f\_n \text{ dVol}\end{bmatrix}$
Average value is still $\bar{\boldsymbol{f}} = \cfrac{1}{\text{vol}(\Omega)}\displaystyle\int\_\Omega f \text{ dVol}$
And *center of mass* of $\Omega$ with mass density function $\delta(\boldsymbol{x})$ is defined to be the point $\bar{\boldsymbol{x}} = \cfrac{1}{\text{vol}(\Omega)}\displaystyle\int\_\Omega \delta(\boldsymbol{x})\boldsymbol{x} \text{ dVol}$
For a rigid body $\Omega$, its *moment of inertia* about an axis $l$ is defined to be $\displaystyle\int\_\Omega \delta(\boldsymbol{x})r^2 \text{ dVol}$, where $r$ is the distance from the point to the axis $l$.
The gravitational force exerted on the origin by a mass $\Omega$ is  $\boldsymbol{F} = G\displaystyle\int\_\Omega \delta(\boldsymbol{x})\cfrac{\boldsymbol{x}}{\|\boldsymbol{x}\|^3} \text{ dVol}$

## 7.5 Determinants and n-Dimensional Volume

### The Determinant
For every $n \geq 1$, there is exactly one function $\mathcal{D}:\underbrace{\mathbb{R}^n\times\cdots\times\mathbb{R}^n}\_{n\text{ times}} \to \mathbb{R}$ that is alternating, multilinear, and has $\mathcal{D}(\boldsymbol{e}\_1, ..., \boldsymbol{e}\_n) = 1$

**Alternating:**<br>
$\mathcal{D}(\boldsymbol{v}\_1, \cdots, \boldsymbol{v}\_i, \cdots, \boldsymbol{v}\_j, \cdots, \boldsymbol{v}\_n) = -\mathcal{D}(\boldsymbol{v}\_1, \cdots, \boldsymbol{v}\_j, \cdots, \boldsymbol{v}\_i, \cdots, \boldsymbol{v}\_n)$

**Multilinear** (linear in all inputs)**:**<br>
$\mathcal{D}(\boldsymbol{v}\_1, \cdots, c\boldsymbol{v}\_i, \cdots, \boldsymbol{v}\_n) = c\mathcal{D}(\boldsymbol{v}\_1, \cdots, \boldsymbol{v}\_i, \cdots, \boldsymbol{v}\_n)$
and
$\mathcal{D}(\boldsymbol{v}\_1, \cdots, \boldsymbol{v}\_i+\boldsymbol{v}\_i', \cdots, \boldsymbol{v}\_n)= \mathcal{D}(\boldsymbol{v}\_1, \cdots, \boldsymbol{v}\_i, \cdots, \boldsymbol{v}\_n) + \mathcal{D}(\boldsymbol{v}\_1, \cdots, \boldsymbol{v}\_i', \cdots, \boldsymbol{v}\_n)$

**Standard:**<br>
$\mathcal{D}(\boldsymbol{e}\_1, ..., \boldsymbol{e}\_n) = 1$
This function is the determinant. For an $n\times n$ matrix $A$, $\det A = \mathcal{D}(\boldsymbol{a}\_1, \cdots, \boldsymbol{a}\_n)$

### Properties
1. If two columns of a matrix are equal, $\det A = 0$
2. Performing row or column operations on a matrix changes the determinant nicely
	- Swapping two rows or columns leaves $\det A' = - \det A$
	- Scaling one row or column by $c$ leaves $\det A' = c \det A$
	- Adding a multiple of one row or column to another leaves $\det A' =  \det A$
3. A is nonsingular $\iff \det A \neq 0$
4. $\det(AB) = \det(A)\det(B)$
5. $\det A^{-1} = \cfrac{1}{\det A}$
6.  $\det A^\intercal = \det A$
7. if A is triangular, $\det A$ is the product of the diagonal entries
8. if $T$ is a linear map, and $\Omega$ is a region, $\text{vol}(T(\Omega)) =
\lvert\det T\rvert\text{vol}(\Omega)$

**Cofactors:**<br>
The cofactor $c\_{i,j}= (-1)^{i+j}\det A\_{i,j}$ , where $A\_{i, j} =$ the matrix formed by removing row $i$ and column $j$.
The cofactor matrix $C\_A$ is the matrix where $(C\_A)\_{i,j} = c\_{i,j}$

**Cramers Rule:**<br>
The $i^{\text{th}}$ coordinate of $\boldsymbol{x}$ which solves $A\boldsymbol{x}=\boldsymbol{b}$ is given by $\boldsymbol{x}\_i =\cfrac{\det B\_i}{\det A}$, where $B\_i$ is the matrix obtained by replacing the ith column of $A$ by $\boldsymbol{b}$

**Inverse matrix "formula"**:<br>
$A^{-1} = \cfrac{1}{\det A}C^\intercal$

**Permutations**:<br>
A permutation $\sigma \in S\_n$ is a bijective mapping $\sigma: \\{1, \cdots,n\\} \to \\{1, \cdots,n\\}$
Set of all permutations is denoted $S\_n$
The sign of a permutation $\text{sign}(\sigma) = \begin{cases}+1&\text{odd \# of exchanges}\\\\-1&\text{even \# of exchanges}\end{cases}$
where $\text{\# of exchanges}$ is the number of swaps required to change the ordered list $(1, \cdots, n)$ to $(\sigma(1), \cdots, \sigma(n))$

### Formulas for the Determinant
**Expansion by cofactors:**<br>
For any fixed row $i$
$\det A =\sum\limits\_{j=1}^n a\_{i,j}c\_{i,j}$
For any fixed column $j$
$\det A =\sum\limits\_{i=1}^n a\_{i,j}c\_{i,j}$

**Sum of row/column permutations:**<br>
$\det A = \sum\limits\_{\sigma \in S\_n}\text{sign}(\sigma)a\_{\sigma(1), 1}a\_{\sigma(2), 2}\cdots a\_{\sigma(n), n}$
$\det A = \sum\limits\_{\sigma \in S\_n}\text{sign}(\sigma)a\_{1, \sigma(1)}a\_{2, \sigma(2)}\cdots a\_{n, \sigma(n)}$

## 7.6 Change of Variables Theorem
Let $\Omega \subset \mathbb{R}^n$ be a region and let there be an open $U$ with $\Omega \subset U$ so that $\boldsymbol{g}: U \to \mathbb{R}^n$ is injective, ${\mathscr{C}^1}$, and has a derivative that is mostly invertible (only fails to be invertible on a set of volume 0).
if $f:\boldsymbol{g}(\Omega) \to \mathbb{R}$ and $(f \circ \boldsymbol{g})|\det D\boldsymbol{g}|: \Omega \to \mathbb{R}$ are both integrable, then $\displaystyle\int\_{\boldsymbol{g}(\Omega)}f(\boldsymbol{y})\text{ dVol}\_\boldsymbol{y} = \displaystyle\int\_{\Omega}(f \circ \boldsymbol{g})(\boldsymbol{x})|\det D\boldsymbol{g}(\boldsymbol{x})|\text{ dVol}\_\boldsymbol{x}$

---
# Chapter 8: Differential Forms and Integration on Manifolds

## 8.2 Differential Forms

### Linear forms
The basis 1-forms $\\{\operatorname{d}\\!x\_1, ..., \operatorname{d}\\!x\_n\\}$ are defined by $\operatorname{d}\\!x\_i(\boldsymbol{v})=v\_i$.
The set of linear maps from $\mathbb{R}^n$ to $\mathbb{R}$ is an $n$ dimensional vector space $(\mathbb{R}^n)^\*$ spanned by the basis 1-forms $\{\operatorname{d} x\_1, ..., \operatorname{d} x\_n\}$. This space is the *dual space* of $\mathbb{R}^n$.
Let $I = (i\_1, ..., i\_k)$ be a multi-index. Using this, we want to define an alternating, multilinear function $\operatorname{d}\\!\boldsymbol{x}\_I : \underbrace{\mathbb{R}^n \times \cdots \times \mathbb{R}^n}\_{k \text{ times}} \to \mathbb{R}$ that takes $k$ vectors in $\mathbb{R}^n$ to $\mathbb{R}$ by $\operatorname{d}\\!\boldsymbol{x}\_I(\boldsymbol{v}\_1, ..., \boldsymbol{v}\_k) =\det V\_I$, where $V\_I$ is formed by taking the $i\_1, ..., i\_k$th rows of the matrix $\begin{bmatrix}\uparrow&&\uparrow \\\\ \boldsymbol{v\_1}&\cdots&\boldsymbol{v}\_k \\\\ \downarrow&&\downarrow\end{bmatrix}$. This calculates the volume of the parallelepiped of the vectors $\boldsymbol{v}\_1, ..., \boldsymbol{v}\_k$ projected onto the $x\_{i\_1}, ..., x\_{i\_k}$ plane.
The set of alternating, multilinear functions from $(\mathbb{R}^n)^k$ to $\mathbb{R}$ is denoted by $\Lambda^k(\mathbb{R}^n)^\*$. This is the dual space of the exterior power of $\mathbb{R}^n$. This space is spanned by the basis k-forms which are given by all $\operatorname{d}\\!\boldsymbol{x}\_I$ where the $(i\_1, ..., i\_k)$ are increasing. The dimension of this space $\dim(\Lambda^k(\mathbb{R}^n)^\*) = \binom{n}{k}$. So every $T \in \Lambda^k(\mathbb{R}^n)^\*$ is described by $\binom{n}{k}$ coefficients $a\_I$ so that $T = \displaystyle\sum\limits\_{I \text{ increasing}}a\_I\operatorname{d}\\!\boldsymbol{x}\_I$.
The wedge product $\wedge: \Lambda^k(\mathbb{R}^n)^\*\times \Lambda^l(\mathbb{R}^n)^\* \to \Lambda^{k+l}(\mathbb{R}^n)^\*$ can be defined in multiple ways for two forms $\omega = \sum\limits\_{I \text{ increasing}}a\_I\operatorname{d}\!\boldsymbol{x}\_I \in \Lambda^k(\mathbb{R}^n)^\*$ and $\eta = \sum\limits\_{J \text{ increasing}}b\_J\operatorname{d}\\!\boldsymbol{x}\_J \in \Lambda^l(\mathbb{R}^n)^\*$.

**1.) Concatenation:**<br>
For the basis forms, we simply concatenate the multi-indices so that $\operatorname{d}\\!\boldsymbol{x}\_I \wedge \operatorname{d}\\!\boldsymbol{x}\_J = \operatorname{d}\\!\boldsymbol{x}\_{(I, J)} =  \operatorname{d}\\!\boldsymbol{x}\_{(i\_1, ..., i\_k, j\_1, ..., j\_l)}$. By linearity then, we have $\omega \wedge \eta = \sum\limits\_{I \text{ increasing}}\sum\limits\_{J \text{ increasing}}(a\_I b\_J)\operatorname{d}\\!\boldsymbol{x}\_{(I,J)}$.

2.) **Alternization of Tensor Product**:<br>
$\omega \wedge \eta = \text{alt}(\omega \otimes \eta)$,
where $\otimes$ is the tensor product which is defined by $(\omega\otimes\eta)(\boldsymbol{v}\_1, ..., \boldsymbol{v}\_{k+l})= \omega(\boldsymbol{v}\_1, ..., \boldsymbol{v}\_k)\cdot\eta(\boldsymbol{v}\_{k+1}, ..., \boldsymbol{v}\_{k+l})$
and $\text{alt}$ is the alternization for a multilinear $\gamma:(\mathbb{R}^n)^k \to \mathbb{R}$ defined by $\text{alt}(\gamma)(\boldsymbol{v}\_1, ..., \boldsymbol{v}\_{k}) = \sum\limits\_{\sigma \in S\_k}\text{sign}(\sigma)\gamma(\boldsymbol{v}\_{\sigma(1)}, ..., \boldsymbol{v}\_{\sigma(k)})$.
Note that every $\operatorname{d}\\!\boldsymbol{x}\_I = \operatorname{d} x\_{i\_1} \wedge \cdots \wedge \operatorname{d} x\_{i\_k}$

**Properties:**<br>
$\omega \in \Lambda^k(\mathbb{R}^n)^\*, \eta \in \Lambda^l(\mathbb{R}^n)^\*, \mu \in \Lambda^m(\mathbb{R}^n)^\*$
1. Bilinear
	- $(\omega + \eta)\wedge \mu = \omega \wedge \mu + \eta \wedge \mu$
	- $(c\omega)\wedge \eta = c(\omega \wedge \eta)$
2. Skew-commutative
	- $\omega \wedge \eta = (-1)^{kl} \eta \wedge \omega$
3. Associative
	- $(\omega \wedge \eta)\wedge \mu = \omega \wedge (\eta \wedge \mu)$

### Differential Forms
A differential form associates a function to each of the $\binom{n}{k}$ basis forms. A differential $k$-form on $\mathbb{R}^n$ is an expression $\omega =\sum\limits\_{I \text{ increasing}}f\_I(\boldsymbol{x})\operatorname{d}\!\boldsymbol{x}\_I$.

**Special cases:**<br>
A 0-form is a smooth function.
An n-form is an expression of the form $\omega = f(\boldsymbol{x})\operatorname{d} x\_1\wedge\cdots\wedge\operatorname{d} x\_n$.
The set of $k$ forms on $\mathbb{R}^n$ forms a vector space $\mathcal{A}^k(\mathbb{R}^n)$

**Properties:**<br>
$\omega \in \mathcal{A}^k(\mathbb{R}^n), \eta \in \mathcal{A}^l(\mathbb{R}^n), \mu \in \mathcal{A}^m(\mathbb{R}^n)$

1. Distributes (when $k=l$)
	- $(\omega + \eta)\wedge \mu = \omega \wedge \mu + \eta \wedge \mu$
2. Skew-commutative
	- $\omega \wedge \eta = (-1)^{kl} \eta \wedge \omega$
3.  Associative
	- $(\omega \wedge \eta)\wedge \mu = \omega \wedge (\eta \wedge \mu)$
4. Associative / commutative (when $k=l=m$)
	- $(\omega + \eta) + \mu = \omega + (\eta + \mu)$
	- $\omega + \eta = \eta + \omega$

### Exterior derivative
The exterior derivative $\operatorname{d}$ allows us to differentiate differential forms. The exterior derivative turns $k$ forms into $k+1$ forms.
For 0-forms, $f: U \subset \mathbb{R}^n \to \mathbb{R}$ we want $\operatorname{d} f(\boldsymbol{x}) = Df(\boldsymbol{x})$ as a linear map $\mathbb{R}^n \to \mathbb{R}$. This motivates us to define $\operatorname{d} f = \sum\limits\_{i=1}^n\cfrac{\partial f}{\partial x\_i}\operatorname{d} x\_i$
For a general $k$ form $\omega = \sum\limits\_I f\_I(\boldsymbol{x})\operatorname{d}\\!\boldsymbol{x}\_I$, we define $\operatorname{d} \omega = \sum\limits\_I \operatorname{d}f\_I\wedge \operatorname{d}\\!\boldsymbol{x}\_I = \sum\limits\_I \sum\limits\_{j=1}^n\cfrac{\partial f}{\partial x\_j}\operatorname{d} x\_j\wedge \operatorname{d} x\_{i\_1}\wedge \cdots \wedge \operatorname{d} x\_{i\_k} $

**Properties:**<br>
$\omega \in \mathcal{A}^k(\mathbb{R}^n), \eta \in \mathcal{A}^l(\mathbb{R}^n), f \in \mathcal{A}^0(\mathbb{R}^n)$ (smooth function)
1. Distributes (when k = l)
	- $\operatorname{d}(\omega + \eta) = \operatorname{d}\omega + \operatorname{d}\eta$
2. Scalar function product rule
	- $\operatorname{d}(f\omega)  = \operatorname{d} f \wedge \omega + f \operatorname{d}\omega$
3. General product rule
	- $\operatorname{d}(\omega \wedge \eta) = \operatorname{d}\omega \wedge \eta + (-1)^k\omega\wedge\operatorname{d}\eta$
4. Derivative twice is 0
	- $\operatorname{d}(\operatorname{d}\omega)=0$

### Pullback
Pullback is a sort of differential-forms generalization of the chain rule.
If (for open $U$) $\boldsymbol{g}: U \subset \mathbb{R}^m \to \mathbb{R}^n$ is smooth, and $\omega \in \mathcal{A}^k(\mathbb{R}^n)$, then we define $\boldsymbol{g}^\*\omega \in \mathcal{A}^k(U)$ in multiple steps
To pull back a 0-form (function), we compose by $\boldsymbol{g}^\*f=f\circ\boldsymbol{g}$
To pull back a basis 1-form, we set $\boldsymbol{g}^\*\operatorname{d} x\_i = \operatorname{d} g\_i = \sum\limits\_{j=1}^m\cfrac{\partial g\_i}{\partial u\_j}\operatorname{d} u\_j$
and to pull back a basis k-form, we set
$\boldsymbol{g}^\*(\operatorname{d} x\_{i\_1} \wedge \cdots \wedge \operatorname{d} x\_{i\_k}) = \boldsymbol{g}^\*\operatorname{d} x\_{i\_1} \wedge \cdots \wedge \boldsymbol{g}^\*\operatorname{d} x\_{i\_k} = \operatorname{d} g\_{i\_1} \wedge \cdots \wedge \operatorname{d} g\_{i\_k} = \operatorname{d} \boldsymbol{g}\_I$
and finally, to take the pullback of a general form, we distribute the pullback so, $\boldsymbol{g}^\*\omega = \boldsymbol{g}^\*\left(\sum\limits\_I f\_I(\boldsymbol{x})\operatorname{d}\\!\boldsymbol{x}\_I\right) = \sum\limits\_I (f\_I\circ\boldsymbol{g})(\boldsymbol{x})\operatorname{d} \boldsymbol{g}\_I = \sum\limits\_I (f\_I\circ\boldsymbol{g})(\boldsymbol{x})\operatorname{d} g\_{i\_1} \wedge \cdots \wedge \operatorname{d} g\_{i\_k}$
This tells us that
$\boldsymbol{g}^\*\operatorname{d}\\!\boldsymbol{x}\_I = \sum\limits\_{J \text{ increasing}} \det[\cfrac{\partial \boldsymbol{g}\_I}{\partial \boldsymbol{u}\_J}]\operatorname{d} \boldsymbol{u}\_J = \sum\limits\_{J \text{ increasing}} \begin{vmatrix} \cfrac{\partial g\_{i\_1}}{\partial u\_{j\_1}} & \cdots & \cfrac{\partial g\_{i\_1}}{\partial u\_{j\_k}} \\\\ \vdots & \ddots& \vdots \\\\ \cfrac{\partial g\_{i\_k}}{\partial u\_{j\_1}} & \cdots & \cfrac{\partial g\_{i\_k}}{\partial u\_{j\_k}}\end{vmatrix}\operatorname{d} u\_{j\_1}\wedge\cdots\wedge\operatorname{d} u\_{j\_k}$
also note that $\boldsymbol{g}^\*(\operatorname{d} \omega) = \operatorname{d}(\boldsymbol{g}^\*\omega)$

### Defining integration
Given an $n$-form $\omega = f(\boldsymbol{x}) = \operatorname{d} x\_1 \wedge \cdots \wedge \operatorname{d} x\_n$ on a region $\Omega \subset \mathbb{R}^n$, we define $\displaystyle\int\_\Omega\omega := \displaystyle \int\_\Omega f\text{ dVol}$
With this definition, we can nicely restate the change of variables theorem in a nice way. If $\boldsymbol{g}: \Omega \subset \mathbb{R}^n \to \mathbb{R}^n$ is smooth, injective, and has $\det(D\boldsymbol{g}) > 0$ on a region $\Omega$, then for any $\omega \in \mathcal{A}^n(\mathbb{R}^n)$ we have$\displaystyle\int\_{\boldsymbol{g}(\Omega)}\omega = \displaystyle \int\_\Omega \boldsymbol{g}^\*\omega$. This motivates us to define integration on a manifold.
If $\boldsymbol{g}: \Omega \subset \mathbb{R}^n \to \mathbb{R}^n$ is smooth, injective, and has $\text{rank}(\det D\boldsymbol{g})=k$ on a region $\Omega$, and only doesn't have this rank on a set of volume 0, then for a parameterized k-dimensional manifold $M=\boldsymbol{g}(\Omega)\subset\mathbb{R}^n$ and for any $\omega \in \mathcal{A}^n(\mathbb{R}^n)$ we define $\displaystyle\int\_{M}\omega := \displaystyle \int\_\Omega \boldsymbol{g}^\*\omega$
This is well defined, and has the same result for different parameterizations (as long as both parameterizations orient $M$ in the same way)

## 8.3 Line Integrals and Green's Theorem
We can associate a 1-form in $\mathbb{R}^n$ ($\omega = \sum f\_i \operatorname{d} x\_i$) with a vector field $\boldsymbol{F}: \mathbb{R}^n \to \mathbb{R}^n$, where $\boldsymbol{F}(\boldsymbol{x}) = \begin{bmatrix}f\_1(\boldsymbol{x})\\\\\vdots\\\\f\_n(\boldsymbol{x})\end{bmatrix}$.  Then, if we have a curve $\mathcal{C}$, parameterized by some function $\boldsymbol{g}: [a, b] \to \mathcal{C}$, we can find out how much *work* is done on a particle traveling on the path $\mathcal{C}$ by the force field $\boldsymbol{F}$.
This work is given by the line integral $\displaystyle\int\_\mathcal{C} \omega = \displaystyle\int\_\mathcal{C} f\_1(\boldsymbol{x})\operatorname{d} x\_1 + ... + f\_n(\boldsymbol{x})\operatorname{d} x\_n$.
To compute this integral, we pull back via the parameterization $\boldsymbol{g}$ to get $\displaystyle\int\_{\boldsymbol{g}([a, b])} \omega = \displaystyle\int\_{[a, b]} \boldsymbol{g}^\*(\omega) = \displaystyle\int\_a^b f\_1(\boldsymbol{g}(t))\; g\_1'(t)\operatorname{d} t + ... + f\_n(\boldsymbol{g}(t))\; g\_n'(t)\operatorname{d} t = \displaystyle\int\_a^b \boldsymbol{F}(\boldsymbol{g}(t))\cdot \nabla \boldsymbol{g}(t) \operatorname{d} t$
It's common notation to let $\boldsymbol{s} = \boldsymbol{g}(t)$ so that $\displaystyle\int\_a^b \boldsymbol{F}(\boldsymbol{g}(t))\cdot \nabla \boldsymbol{g}(t) \operatorname{d} t = \displaystyle\int\_a^b \boldsymbol{F}(\boldsymbol{s})\cdot \operatorname{d} \boldsymbol{s}$

### F.T.C for line integrals
For a 1-form $\omega$, if we can find a potential 0-form $f$ so that $\omega = \operatorname{d} f$, then for any curve $\mathcal{C}$ with endpoints $\boldsymbol{A}, \boldsymbol{B} \in \mathcal{C}$, we have $\displaystyle\int\_\mathcal{C} \omega = f(\boldsymbol{A})- f(\boldsymbol{B})$. In traditional vector notation, if we can find an $f$ so that the force field $\boldsymbol{F} = \nabla f$, then for a curve $\mathcal{C}$ parameterized by a $\boldsymbol{g}: [a, b] \to \mathbb{R}^n$, we have $\displaystyle\int\_\mathcal{C} \boldsymbol{F}(\boldsymbol{s})\cdot \operatorname{d} \boldsymbol{s} = f(\boldsymbol{g}(b)) - f(\boldsymbol{g}(a))$.

### Exactness
A 1-form $\omega$ is exact if any of the following equivalent statements are true:
*(The endpoints of a curve $\mathcal{C}$ are denoted $\boldsymbol{A}$ and $\boldsymbol{B}$)*
1. $\omega = \operatorname{d} f$ for some 0-form $f$
2. For every closed curve (when $\boldsymbol{A}= \boldsymbol{B}$), we have $\displaystyle\int\_\mathcal{C}\omega = 0$
3. $\displaystyle\int\_\mathcal{C}\omega$ is path-independent. ( $\displaystyle\int\_\mathcal{C}\omega$ =  $\displaystyle\int\_\mathcal{C'}\omega$ if $\mathcal{C}$ and $\mathcal{C}'$ have the same endpoints)

### Closedness
A 1-form $\omega$ is closed if $\operatorname{d}\omega = 0$

### Simply Connected Regions
- exactness $\implies$ closedness
  - because $\operatorname{d}^2 = 0$
- closedness $\;\not\\!\\!\\!\implies$ exactness
  - *except on special regions called "simply connected regions"
- $X \subset \mathbb{R}^n$ is called *simply connected* if any pair of points can be joined by a piecewise ${\mathscr{C}^1}$ path (connected), and every closed curve which doesn't intersect itself (every simple curve) in $X$ can be continuously shrunk to a point in $X$.
  - *Intuition*: the subset has no holes
  - Think about an open set. You can draw any closed curve and shrink it. Think about an open set with a hole. If you draw a curve around the hole, you can't shrink the curve to a point anymore
- If $\omega$ is a smooth, closed 1-form on a simply connected region $\Omega \subset \mathbb{R}^2$, then $\omega$ is exact.

### Green's Theorem
**Green's Theorem for a Rectangle**:<br>
For a rectangle $R \subset \mathbb{R}^2$, and a 1-form $\omega$ on $R$.
$\displaystyle\int\_{\partial R}\omega = \displaystyle\int\_{R}\operatorname{d}\omega$(Where $\partial R$ is counterclockwise)

**Classical Green's Theorem**:<br>
For an $\boldsymbol{F}: \mathbb{R}^2 \to \mathbb{R}^2$, where $\boldsymbol{F}(x, y) = \begin{bmatrix}F\_1(x, y)\\\\F\_2(x, y)\end{bmatrix}$. And for a region $D$ bounded by $C$. Then
$\displaystyle\int\_C F\_1(x,y)\operatorname{d} x + F\_2(x,y)\operatorname{d} y = \iint\_D \left( \frac{\partial F\_2} {\partial x} - \frac{\partial F\_1}{\partial y} \right) \operatorname{d} x \operatorname{d} y$

**Slight Generalization**:<br>
- If $S \subset \mathbb{R}^2$ is parameterized by a rectangle, then $\displaystyle\int\_{\partial S}\omega = \displaystyle\int\_{S}\operatorname{d}\omega$.
- Green's Theorem holds for any region $S$ that can be decomposed as a finite union of parameterized rectangles which overlap only along their boundaries.

### Winding Number
Something fun we can do is count the number of times a parameterized curve $\gamma$ "winds" around the origin. We define $\text{wind}(\gamma) = \cfrac{1}{2\pi} \displaystyle\int\_\gamma\cfrac{-y}{x^2+y^2}\operatorname{d} x + \cfrac{x}{x^2+y^2} \operatorname{d} y$, and this counts the number of times a curve loops counterclockwise around the origin.
## 8.4 Surface Integrals and Flux

### Parameterized Surface
We can work with special 2-dimensional surfaces in $\mathbb{R}^3$, like spheres, toruses, cylinders, etc.

Suppose we have a surface $S = \boldsymbol{g}(U)$ parameterized by an injective function $\boldsymbol{g}: U \subset \mathbb{R}^2 \to \mathbb{R}^n$ (for a bounded, open $U$) that alway has $\rank(D\boldsymbol{g}(\boldsymbol{x})) = 2$. Then $S$ is a *parameterized surface*.

We define the integral of a 2-form $\omega \in \mathcal{A}^2(\mathbb{R}^n)$ over this parameterized surface to be $\displaystyle\int\_S \omega = \displaystyle\int\_U \boldsymbol{g}^\*\omega$.

### Orientation
For each tangent plane of $S$, find a positively oriented basis $(\boldsymbol{u}, \boldsymbol{v})$ (i.e. one where $\det(\boldsymbol{u}, \boldsymbol{v}) > 0$). An orientation for $S$ is a continuously varying notion of what a positively oriented basis for the plane at each point of $S$ should be.

An orientation can be determined in multiple ways.

$S$ has an orientation iff we can choose various parameterizations $\boldsymbol{g}\_i$ of subsets $S\_i \subset S$ such that $\bigcup\limits\_i S\_i = S$ and that at every point $\boldsymbol{x} \in S$ the corresponding $\boldsymbol{g}\_i$ has $\det(\cfrac{\partial\boldsymbol{g}\_i}{\partial u\_1}, \cfrac{\partial\boldsymbol{g}\_i}{\partial u\_1}) > 0$ (the tangent vectors of the parameterization form a positively oriented basis).

An orientation can also be determined if we have a 2-form $\omega$ which is never 0 on $S$. This is because at each point $\boldsymbol{x} \in S$, we can pick tangent vectors $\boldsymbol{u}, \boldsymbol{v}$ so that $\omega(\boldsymbol{a})(\boldsymbol{u}, \boldsymbol{v}) > 0$, which defines a continuously varying notion of what a positively oriented basis for the plane should be.


### Area Forms
For an oriented surface $S$, it's oriented area 2-form $\sigma$ assigns at each point $\boldsymbol{x} \in S$ the signed area of the parallelogram that the tangent vectors at that point span.


In $\mathbb{R}^3$ (where $\boldsymbol{x} = (x, y, z)$), the area 2-form gives the *surface area* of a surface $S$. If at each point $\boldsymbol{x} \in S$, the surface $S$ has outward pointing unit normal given by $\boldsymbol{n}(\boldsymbol{x}) = \begin{bmatrix}n\_1(\boldsymbol{x})\\\\n\_2(\boldsymbol{x})\\\\n\_3(\boldsymbol{x})\end{bmatrix}$, then the corresponding area 2-form is $\sigma = n\_1(\boldsymbol{x}) \operatorname{d}\\! y \wedge \operatorname{d}\\! z + n\_2(\boldsymbol{x}) \operatorname{d}\\! z \wedge \operatorname{d}\\! x + n\_3(\boldsymbol{x}) \operatorname{d}\\! x \wedge \operatorname{d}\\! y$. This works because $\boldsymbol{n}$ is a unit vector, so the area of the plane spanned by tangent vectors $\boldsymbol{u}$ and $\boldsymbol{v}$ is given by $\det(\begin{bmatrix}\uparrow&\uparrow&\uparrow\\\\\boldsymbol{n}&\boldsymbol{u}&\boldsymbol{v}\\\\\downarrow&\downarrow&\downarrow\end{bmatrix})$.

<!-- ### Flux
// TODO

## 8.5 Stoke's Theorem
// TODO
## 8.6 Applications to Physics

## 8.7 Applications to Topology

---
# Chapter 9: Eigenvalues, Eigenvectors, and Applications

## 9.1 Linear Transformations and Change of Basis
// TODO
## 9.2 Eigenvalues, Eigenvectors, and Diagonalizability
// TODO
## 9.3 Difference Equations and Ordinary Differential Equations
// TODO
## 9.4 The Spectral Theorem
// TODO -->
